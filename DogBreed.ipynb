{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DogBreed.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGOFZksINtsi"
      },
      "source": [
        "### **INITIALIZATION:**\n",
        "- I use these three lines of code on top of my each notebooks because it will help to prevent any problems while reloading the same project. And the third line of code helps to make visualization within the notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiggu5siFraW"
      },
      "source": [
        "#@ INITIALIZATION:\n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIkWvN-_OF19"
      },
      "source": [
        "**DOWNLOADING LIBRARIES AND DEPENDENCIES:**\n",
        "- I have downloaded all the libraries and dependencies required for the project in one particular cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNX5amwWOD8u"
      },
      "source": [
        "#@ DOWNLOADING THE LIBRARIES AND DEPENDENCIES:\n",
        "# !pip install -U d2l\n",
        "\n",
        "import os, collections, math\n",
        "import shutil\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "from d2l import torch as d2l\n",
        "\n",
        "PROJECT_ROOT_DIR = \".\"\n",
        "ID = \"RECOG\"\n",
        "IMAGE_PATH = os.path.join(PROJECT_ROOT_DIR, \"Images\", ID)\n",
        "if not os.path.isdir(IMAGE_PATH):\n",
        "    os.makedirs(IMAGE_PATH)\n",
        "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
        "  path = os.path.join(IMAGE_PATH, fig_id + \".\" + fig_extension)\n",
        "  print(\"Saving Figure\", fig_id)\n",
        "  if tight_layout:\n",
        "    plt.tight_layout()\n",
        "  plt.savefig(path, format=fig_extension, dpi=resolution)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJjgJJAyOg7f"
      },
      "source": [
        "### **OBTAINING AND ORGANIZING THE DATASET:**\n",
        "- I have used google colab for this project so the process of downloading and reading the data might be different in other platforms. I will use [Dog Breed Identification](https://www.kaggle.com/c/dog-breed-identification) for this project. The dataset is divided into training set and test set. There are 120 breeds of dogs in the training dataset including Labradors, Poodles, Dachshunds, Samoyeds, Huskies, Chihuahuas and Yorkshire Terriers.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6cQwPBHOayK"
      },
      "source": [
        "#@ OBTAINING THE DATASET: \n",
        "d2l.DATA_HUB[\"DOG\"] = (d2l.DATA_URL + \"kaggle_dog_tiny.zip\", \n",
        "                       '0cb91d09b814ecdc07b50f31f8dcad3e81d6a86d')                         # Initializing the Dataset. \n",
        "demo = True                                               \n",
        "if demo: data_dir = d2l.download_extract(\"DOG\")                                            # Initialization. \n",
        "else: data_dir = os.path.join(\"..\", \"data\", \"dog-breed-identification\")                    # Initialization. "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzMS_FM-RDk3"
      },
      "source": [
        "**ORGANIZING THE DATASET:**\n",
        "- I will organize the datasets to facilitate model training and testing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G9wPYelxQjvK",
        "outputId": "0c728ba3-7396-4166-f87a-b27229dbe868"
      },
      "source": [
        "#@ ORGANIZING THE DATASET: \n",
        "def read_csv_labels(fname):                                                             # Returning names to Labels. \n",
        "  with open(fname, \"r\") as f:\n",
        "    lines = f.readlines()[1:]                                                           # Reading Lines. \n",
        "  tokens = [l.rstrip().split(\",\") for l in lines]\n",
        "  return dict(((name, label) for name, label in tokens))\n",
        "labels = read_csv_labels(os.path.join(data_dir, \"labels.csv\"))                          # Implementation. \n",
        "print(f\"Training Examples: {len(labels)}\")                                              # Number of Training Examples. \n",
        "print(f\"Classes: {len(set(labels.values()))}\")                                          # Number of Classes."
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Examples: 1000\n",
            "Classes: 120\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXsaBhJHR4_E"
      },
      "source": [
        "#@ ORGANIZING THE DATASET: \n",
        "def copyfile(filename, target_dir):                                                      # Copying File into Target Directory. \n",
        "  os.makedirs(target_dir, exist_ok=True)\n",
        "  shutil.copy(filename, target_dir)\n",
        "#@ ORGANIZING THE DATASET: \n",
        "def reorg_train_valid(data_dir, labels, valid_ratio):\n",
        "  n = collections.Counter(labels.values()).most_common()[-1][1]                          # Number of examples per class. \n",
        "  n_valid_per_label = max(1, math.floor(n * valid_ratio))\n",
        "  label_count = {}\n",
        "  for train_file in os.listdir(os.path.join(data_dir, \"train\")):\n",
        "    label = labels[train_file.split(\".\")[0]]\n",
        "    fname = os.path.join(data_dir, \"train\", train_file)\n",
        "    copyfile(fname, os.path.join(data_dir, \"train_valid_test\", \"train_valid\", label))    # Copy to Train Valid. \n",
        "    if label not in label_count or label_count[label] < n_valid_per_label:\n",
        "      copyfile(fname, os.path.join(data_dir, \"train_valid_test\", \"valid\", label))        # Copy to Valid. \n",
        "      label_count[label] = label_count.get(label, 0) + 1\n",
        "    else: \n",
        "      copyfile(fname, os.path.join(data_dir, \"train_valid_test\", \"train\", label))        # Copy to Train. \n",
        "  return n_valid_per_label"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRET8DroSX32"
      },
      "source": [
        "- The reorg test function is used to organize the testing set to facilitate the reading during prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycXWFidvSUDl"
      },
      "source": [
        "#@ ORGANIZING THE DATASET: \n",
        "def reorg_test(data_dir):                                                           # Initialization. \n",
        "  for test_file in os.listdir(os.path.join(data_dir, \"test\")):\n",
        "    copyfile(os.path.join(data_dir, \"test\", test_file), \n",
        "             os.path.join(data_dir, \"train_valid_test\", \"test\", \"unknown\"))         # Implementation of Function."
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XeIRwuOqShCR"
      },
      "source": [
        "#@ OBTAINING AND ORGANIZING THE DATASET: \n",
        "def reorg_dog_data(data_dir, valid_ratio):                                          # Obtaining and Organizing the Dataset. \n",
        "  labels = read_csv_labels(os.path.join(data_dir, \"labels.csv\"))                    # Implementation of Function. \n",
        "  reorg_train_valid(data_dir, labels, valid_ratio)                                  # Implementation of Function. \n",
        "  reorg_test(data_dir)                                                              # Implementation of Function."
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADnmGcSqSxwW"
      },
      "source": [
        "#@ INITIALIZING THE PARAMETERS: \n",
        "batch_size = 4 if demo else 128                                                     # Initializing Batchsize. \n",
        "valid_ratio = 0.1                                                                   # Initialization. \n",
        "reorg_dog_data(data_dir, valid_ratio)                                               # Obtaining and Organizing the Dataset."
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNSU3VsmTD3G"
      },
      "source": [
        "### **IMAGE AUGMENTATION:**\n",
        "- I will use image augmentation to cope with overfitting. The images are flipped at random and normalized."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjNRF30GS3te"
      },
      "source": [
        "#@ IMPLEMENTATION OF IMAGE AUGMENTATION: TRAINING DATASET: \n",
        "transform_train = torchvision.transforms.Compose([                                                  # Initialization. \n",
        "                  torchvision.transforms.RandomResizedCrop(224, scale=(0.08, 1.0),                  # Cropping and Resizing. \n",
        "                                                           ratio=(3.0 / 4.0, 4.0 / 3.0)),\n",
        "                  torchvision.transforms.RandomHorizontalFlip(),                                    # Randomly Flipping Image.  \n",
        "                  torchvision.transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4), # Changing Brightness, Contrast and Saturation.\n",
        "                  torchvision.transforms.ToTensor(),                                                # Adding Random Noise. \n",
        "                  torchvision.transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], \n",
        "                                                   std=[0.2023, 0.1994, 0.2010])])                  # Normalization of RGB Channels.\n",
        "#@ IMPLEMENTATION OF IMAGE AUGMENTATION: TEST DATASET: \n",
        "transform_test = torchvision.transforms.Compose([                                                   # Initialization. \n",
        "                 torchvision.transforms.Resize(225),                                                # Resizing the Images. \n",
        "                 torchvision.transforms.CenterCrop(224),                                            # Cropping the Images. \n",
        "                 torchvision.transforms.ToTensor(),                                                 # Adding Random Noise. \n",
        "                 torchvision.transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], \n",
        "                                                   std=[0.2023, 0.1994, 0.2010])])                  # Normalization of RGB Channels."
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuzCdcT-XrGl"
      },
      "source": [
        "### **READING THE DATASET:**\n",
        "- I will create the image folder dataset instance to read the organized dataset containing original image files where each example includes the image and label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzWXzUePWVHs"
      },
      "source": [
        "#@ READING THE DATASET: \n",
        "train_ds, train_valid_ds = [torchvision.datasets.ImageFolder(\n",
        "    os.path.join(data_dir, \"train_valid_test\", folder), \n",
        "    transform = transform_train) for folder in [\"train\", \"train_valid\"]]                    # Initializing Training Dataset. \n",
        "#@ READING THE DATASET: \n",
        "valid_ds, test_ds = [torchvision.datasets.ImageFolder(\n",
        "    os.path.join(data_dir, \"train_valid_test\", folder), \n",
        "    transform = transform_test) for folder in [\"valid\", \"test\"]]                            # Initializing Test Dataset."
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUjEr-yQYNK2"
      },
      "source": [
        "#@ IMPLEMENTATION OF DATALOADER: \n",
        "train_iter, train_valid_iter = [torch.utils.data.DataLoader(\n",
        "    dataset, batch_size, shuffle=True, drop_last=True) for dataset in (train_ds, \n",
        "                                                                       train_valid_ds)]      # Implementation of DataLoader. \n",
        "valid_iter = torch.utils.data.DataLoader(valid_ds, batch_size, shuffle=False,drop_last=True) # Implementation of DataLoader. \n",
        "test_iter = torch.utils.data.DataLoader(test_ds, batch_size, shuffle=False, drop_last=False) # Implementation of DataLoader.  "
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4RFoPHJYzYq"
      },
      "source": [
        "### **DEFINING THE MODEL:**\n",
        "- I will use the pretrained ResNet34 model. I will use the input of the pretrained model output layer which is the extracted features. I will replace the output layer with a small custom output layer that can be trained. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_d5HkwBYxCw"
      },
      "source": [
        "#@ DEFINING THE MODEL:                                                                      # Function for Initializing the Model.\n",
        "def get_net(devices):                                                                   \n",
        "  finetune_net = nn.Sequential()                                                            # Initializing the Sequential Model. \n",
        "  finetune_net.features = torchvision.models.resnet34(pretrained=True)                      # Initializing the Pretrained RESNET Model. \n",
        "  finetune_net.output_new = nn.Sequential(nn.Linear(1000, 256), nn.ReLU(), \n",
        "                                          nn.Linear(256, 120))                              # Defining the Output Layer. \n",
        "  finetune_net = finetune_net.to(devices[0])\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}