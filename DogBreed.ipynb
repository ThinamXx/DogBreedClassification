{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DogBreed.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGOFZksINtsi"
      },
      "source": [
        "### **INITIALIZATION:**\n",
        "- I use these three lines of code on top of my each notebooks because it will help to prevent any problems while reloading the same project. And the third line of code helps to make visualization within the notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiggu5siFraW"
      },
      "source": [
        "#@ INITIALIZATION:\n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIkWvN-_OF19"
      },
      "source": [
        "**DOWNLOADING LIBRARIES AND DEPENDENCIES:**\n",
        "- I have downloaded all the libraries and dependencies required for the project in one particular cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNX5amwWOD8u"
      },
      "source": [
        "#@ DOWNLOADING THE LIBRARIES AND DEPENDENCIES:\n",
        "# !pip install -U d2l\n",
        "\n",
        "import os, collections, math\n",
        "import shutil\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "from d2l import torch as d2l\n",
        "\n",
        "PROJECT_ROOT_DIR = \".\"\n",
        "ID = \"RECOG\"\n",
        "IMAGE_PATH = os.path.join(PROJECT_ROOT_DIR, \"Images\", ID)\n",
        "if not os.path.isdir(IMAGE_PATH):\n",
        "    os.makedirs(IMAGE_PATH)\n",
        "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
        "  path = os.path.join(IMAGE_PATH, fig_id + \".\" + fig_extension)\n",
        "  print(\"Saving Figure\", fig_id)\n",
        "  if tight_layout:\n",
        "    plt.tight_layout()\n",
        "  plt.savefig(path, format=fig_extension, dpi=resolution)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJjgJJAyOg7f"
      },
      "source": [
        "### **OBTAINING AND ORGANIZING THE DATASET:**\n",
        "- I have used google colab for this project so the process of downloading and reading the data might be different in other platforms. I will use [Dog Breed Identification](https://www.kaggle.com/c/dog-breed-identification) for this project. The dataset is divided into training set and test set. There are 120 breeds of dogs in the training dataset including Labradors, Poodles, Dachshunds, Samoyeds, Huskies, Chihuahuas and Yorkshire Terriers.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6cQwPBHOayK"
      },
      "source": [
        "#@ OBTAINING THE DATASET: \n",
        "d2l.DATA_HUB[\"DOG\"] = (d2l.DATA_URL + \"kaggle_dog_tiny.zip\", \n",
        "                       '0cb91d09b814ecdc07b50f31f8dcad3e81d6a86d')                         # Initializing the Dataset. \n",
        "demo = True                                               \n",
        "if demo: data_dir = d2l.download_extract(\"DOG\")                                            # Initialization. \n",
        "else: data_dir = os.path.join(\"..\", \"data\", \"dog-breed-identification\")                    # Initialization. "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzMS_FM-RDk3"
      },
      "source": [
        "**ORGANIZING THE DATASET:**\n",
        "- I will organize the datasets to facilitate model training and testing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G9wPYelxQjvK",
        "outputId": "2dbe4ecb-2010-4c14-d541-05394bb42cbc"
      },
      "source": [
        "#@ ORGANIZING THE DATASET: \n",
        "def read_csv_labels(fname):                                                             # Returning names to Labels. \n",
        "  with open(fname, \"r\") as f:\n",
        "    lines = f.readlines()[1:]                                                           # Reading Lines. \n",
        "  tokens = [l.rstrip().split(\",\") for l in lines]\n",
        "  return dict(((name, label) for name, label in tokens))\n",
        "labels = read_csv_labels(os.path.join(data_dir, \"labels.csv\"))                          # Implementation. \n",
        "print(f\"Training Examples: {len(labels)}\")                                              # Number of Training Examples. \n",
        "print(f\"Classes: {len(set(labels.values()))}\")                                          # Number of Classes."
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Examples: 1000\n",
            "Classes: 120\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXsaBhJHR4_E"
      },
      "source": [
        "#@ ORGANIZING THE DATASET: \n",
        "def copyfile(filename, target_dir):                                                      # Copying File into Target Directory. \n",
        "  os.makedirs(target_dir, exist_ok=True)\n",
        "  shutil.copy(filename, target_dir)\n",
        "#@ ORGANIZING THE DATASET: \n",
        "def reorg_train_valid(data_dir, labels, valid_ratio):\n",
        "  n = collections.Counter(labels.values()).most_common()[-1][1]                          # Number of examples per class. \n",
        "  n_valid_per_label = max(1, math.floor(n * valid_ratio))\n",
        "  label_count = {}\n",
        "  for train_file in os.listdir(os.path.join(data_dir, \"train\")):\n",
        "    label = labels[train_file.split(\".\")[0]]\n",
        "    fname = os.path.join(data_dir, \"train\", train_file)\n",
        "    copyfile(fname, os.path.join(data_dir, \"train_valid_test\", \"train_valid\", label))    # Copy to Train Valid. \n",
        "    if label not in label_count or label_count[label] < n_valid_per_label:\n",
        "      copyfile(fname, os.path.join(data_dir, \"train_valid_test\", \"valid\", label))        # Copy to Valid. \n",
        "      label_count[label] = label_count.get(label, 0) + 1\n",
        "    else: \n",
        "      copyfile(fname, os.path.join(data_dir, \"train_valid_test\", \"train\", label))        # Copy to Train. \n",
        "  return n_valid_per_label"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRET8DroSX32"
      },
      "source": [
        "- The reorg test function is used to organize the testing set to facilitate the reading during prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycXWFidvSUDl"
      },
      "source": [
        "#@ ORGANIZING THE DATASET: \n",
        "def reorg_test(data_dir):                                                           # Initialization. \n",
        "  for test_file in os.listdir(os.path.join(data_dir, \"test\")):\n",
        "    copyfile(os.path.join(data_dir, \"test\", test_file), \n",
        "             os.path.join(data_dir, \"train_valid_test\", \"test\", \"unknown\"))         # Implementation of Function."
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XeIRwuOqShCR"
      },
      "source": [
        "#@ OBTAINING AND ORGANIZING THE DATASET: \n",
        "def reorg_dog_data(data_dir, valid_ratio):                                          # Obtaining and Organizing the Dataset. \n",
        "  labels = read_csv_labels(os.path.join(data_dir, \"labels.csv\"))                    # Implementation of Function. \n",
        "  reorg_train_valid(data_dir, labels, valid_ratio)                                  # Implementation of Function. \n",
        "  reorg_test(data_dir)                                                              # Implementation of Function."
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADnmGcSqSxwW"
      },
      "source": [
        "#@ INITIALIZING THE PARAMETERS: \n",
        "batch_size = 4 if demo else 128                                                     # Initializing Batchsize. \n",
        "valid_ratio = 0.1                                                                   # Initialization. \n",
        "reorg_dog_data(data_dir, valid_ratio)                                               # Obtaining and Organizing the Dataset."
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNSU3VsmTD3G"
      },
      "source": [
        "### **IMAGE AUGMENTATION:**\n",
        "- I will use image augmentation to cope with overfitting. The images are flipped at random and normalized."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjNRF30GS3te"
      },
      "source": [
        "#@ IMPLEMENTATION OF IMAGE AUGMENTATION: TRAINING DATASET: \n",
        "transform_train = torchvision.transforms.Compose([                                                  # Initialization. \n",
        "                  torchvision.transforms.RandomResizedCrop(224, scale=(0.08, 1.0),                  # Cropping and Resizing. \n",
        "                                                           ratio=(3.0 / 4.0, 4.0 / 3.0)),\n",
        "                  torchvision.transforms.RandomHorizontalFlip(),                                    # Randomly Flipping Image.  \n",
        "                  torchvision.transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4), # Changing Brightness, Contrast and Saturation.\n",
        "                  torchvision.transforms.ToTensor(),                                                # Adding Random Noise. \n",
        "                  torchvision.transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], \n",
        "                                                   std=[0.2023, 0.1994, 0.2010])])                  # Normalization of RGB Channels.\n",
        "#@ IMPLEMENTATION OF IMAGE AUGMENTATION: TEST DATASET: \n",
        "transform_test = torchvision.transforms.Compose([                                                   # Initialization. \n",
        "                 torchvision.transforms.Resize(225),                                                # Resizing the Images. \n",
        "                 torchvision.transforms.CenterCrop(224),                                            # Cropping the Images. \n",
        "                 torchvision.transforms.ToTensor(),                                                 # Adding Random Noise. \n",
        "                 torchvision.transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], \n",
        "                                                   std=[0.2023, 0.1994, 0.2010])])                  # Normalization of RGB Channels."
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuzCdcT-XrGl"
      },
      "source": [
        "### **READING THE DATASET:**\n",
        "- I will create the image folder dataset instance to read the organized dataset containing original image files where each example includes the image and label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzWXzUePWVHs"
      },
      "source": [
        "#@ READING THE DATASET: \n",
        "train_ds, train_valid_ds = [torchvision.datasets.ImageFolder(\n",
        "    os.path.join(data_dir, \"train_valid_test\", folder), \n",
        "    transform = transform_train) for folder in [\"train\", \"train_valid\"]]                    # Initializing Training Dataset. \n",
        "#@ READING THE DATASET: \n",
        "valid_ds, test_ds = [torchvision.datasets.ImageFolder(\n",
        "    os.path.join(data_dir, \"train_valid_test\", folder), \n",
        "    transform = transform_test) for folder in [\"valid\", \"test\"]]                            # Initializing Test Dataset."
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUjEr-yQYNK2"
      },
      "source": [
        "#@ IMPLEMENTATION OF DATALOADER: \n",
        "train_iter, train_valid_iter = [torch.utils.data.DataLoader(\n",
        "    dataset, batch_size, shuffle=True, drop_last=True) for dataset in (train_ds, \n",
        "                                                                       train_valid_ds)]      # Implementation of DataLoader. \n",
        "valid_iter = torch.utils.data.DataLoader(valid_ds, batch_size, shuffle=False,drop_last=True) # Implementation of DataLoader. \n",
        "test_iter = torch.utils.data.DataLoader(test_ds, batch_size, shuffle=False, drop_last=False) # Implementation of DataLoader.  "
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4RFoPHJYzYq"
      },
      "source": [
        "### **DEFINING THE MODEL:**\n",
        "- I will use the pretrained ResNet34 model. I will use the input of the pretrained model output layer which is the extracted features. I will replace the output layer with a small custom output layer that can be trained. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_d5HkwBYxCw"
      },
      "source": [
        "#@ DEFINING THE MODEL:                                                                      # Function for Initializing the Model.\n",
        "def get_net(devices):                                                                   \n",
        "  finetune_net = nn.Sequential()                                                            # Initializing the Sequential Model. \n",
        "  finetune_net.features = torchvision.models.resnet34(pretrained=True)                      # Initializing the Pretrained RESNET Model. \n",
        "  finetune_net.output_new = nn.Sequential(nn.Linear(1000, 256), nn.ReLU(), \n",
        "                                          nn.Linear(256, 120))                              # Defining the Output Layer. \n",
        "  finetune_net = finetune_net.to(devices[0])\n",
        "  for param in finetune_net.features.parameters():\n",
        "    param.requires_grad = False\n",
        "  return finetune_net"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhO6ImYoWNPG"
      },
      "source": [
        "- I will first use the member variable features to obtain the input of the pretrained model output layer which is the extracted features. Then I will use the features as the input for our small custom output network and compute the output. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UsRaveRV7Nz"
      },
      "source": [
        "#@ DEFINING THE LOSS FUNCTION:\n",
        "loss = nn.CrossEntropyLoss(reduction=\"none\")                                                # Initializing CrossEntropy Loss. \n",
        "def evaluate_loss(data_iter, net, devices):                                                 # Function for Evaluating Loss. \n",
        "  l_sum, n = 0.0, 0                                                                         # Initialization. \n",
        "  for features, labels in data_iter: \n",
        "    features, labels = features.to(devices[0]), labels.to(devices[0])                       # Converting into GPU Tensors. \n",
        "    outputs = net(features)                                                                 # Implementation of Neural Network Model. \n",
        "    l = loss(outputs, labels)                                                               # Calculating Loss Function. \n",
        "    l_sum += l.sum()\n",
        "    n += labels.numel()\n",
        "  return l_sum / n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7l_kfbHcml6"
      },
      "source": [
        "**DEFINING THE TRAINING FUNCTION:**\n",
        "- I will define model training function train here. I will select the model and tune hyperparameters according to the model performance on the validation set. The model training function train only trains the small custom output network. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rL-mIqakd_Tv"
      },
      "source": [
        "#@ DEFINING THE TRAINING FUNCTION: \n",
        "def train(net, train_iter, valid_iter, num_epochs, lr, wd, devices, lr_period, \n",
        "          lr_decay):                                                                          # Defining Training Function. \n",
        "  net = nn.DataParallel(net, device_ids=devices).to(devices[0])                               # Initializing the Neural Network. \n",
        "  trainer = torch.optim.SGD((param for param in net.parameters() if param.requires_grad), \n",
        "                            lr=lr, momentum=0.9, weight_decay=wd)                             # Initializing the SGD Optimizer. \n",
        "  scheduler = torch.optim.lr_scheduler.StepLR(trainer, lr_period, lr_decay)                   # Initializing Learning Rate Scheduler. \n",
        "  num_batches, timer = len(train_iter), d2l.Timer()                                           # Initializing the Parameters. \n",
        "  animator = d2l.Animator(xlabel=\"epoch\", xlim=[1, num_epochs],\n",
        "                          legend=[\"train loss\", \"valid loss\"])                                # Initializing the Animation. \n",
        "  for epoch in range(num_epochs):\n",
        "    metric = d2l.Accumulator(2)                                                               # Initializing the Accumulator. \n",
        "    for i, (features, labels) in enumerate(train_iter):\n",
        "      timer.start()                                                                           # Starting the Timer. \n",
        "      features, labels = features.to(devices[0]), labels.to(devices[0])                       # Converting into GPU enabled Tensors. \n",
        "      trainer.zero_grad()                                                                     # Initializing the Zero Gradients. \n",
        "      output = net(features)                                                                  # Implementation of Neural Networks. \n",
        "      l = loss(output, labels).sum()                                                          # Calculating the Loss. \n",
        "      l.backward()                                                                            # Initializing the Back Propagation. \n",
        "      trainer.step()                                                                          # Optimizing Loss Function. \n",
        "      metric.add(l, labels.shape[0])                                                          # Accumulating the Metrics. \n",
        "      timer.stop()                                                                            # Stopping the Timer. \n",
        "      if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\n",
        "        animator.add(epoch + (i + 1) / num_batches, (metric[0] / metric[1], None))            # Implementation of Animation. \n",
        "    if valid_iter is not None: \n",
        "      valid_loss = evaluate_loss(valid_iter, net, devices)                                    # Calculating the Loss. \n",
        "      animator.add(epoch + 1, (None, valid_loss))                                             # Implementation of Animation. \n",
        "    scheduler.step()                                                                          # Optimization of the Model. \n",
        "  if valid_iter is not None: \n",
        "    print(f\"Train loss {metric[0] / metric[1]:.3f},\"                                          # Inspecting the Loss. \n",
        "          f\"Valid loss {valid_loss:.3f}\")                                                     # Inspecting the Validation Loss. \n",
        "  else:\n",
        "    print(f\"Train loss {metric[0] / metric[1]:.3f}\")                                          # Inspecting the Loss. \n",
        "  print(f\"{metric[1] * num_epochs / timer.sum():.1f} examples/sec\"\n",
        "        f\"on {str(devices)}\")"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgU5xK29lvCE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}